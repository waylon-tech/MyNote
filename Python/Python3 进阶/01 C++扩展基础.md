## 目录

[toc]

## 1 示例介绍

作为 C++ 扩展的基础，本篇**围绕一个示例**说明 C++ (and CUDA) 的扩展流程。

假定有一种新的循环单元 (recurrent unit)，它类似于 LSTM 但**没有遗忘门**，并使用**指数线性单元** (Exponential Linear Unit, ELU) 作为内部的激活函数。因为永不遗忘，因此将其称为<u>长期记忆</u> (Long-Long-Term-Memory，LLTM) 单元。

LLTMs 与经典的 vanilla LSTM 非常不同以至难以借用 PyTorch 的 `LSTMCell`，实现它的最简单方法是扩展 `torch.nn.Module`：

```python
class LLTM(torch.nn.Module):
    def __init__(self, input_features, state_size):
        """构造函数"""
        super(LLTM, self).__init__()
        self.input_features = input_features	# 输入特征维度
        self.state_size = state_size			# 隐层维度 1/3
        # 3 * state_size for input gate, output gate and candidate cell gate.
        # input_features + state_size because we will multiply with [input, h].
        self.weights = torch.nn.Parameter(torch.empty(3 * state_size, input_features + state_size))
        self.bias = torch.nn.Parameter(torch.empty(3 * state_size))
        self.reset_parameters()

    def reset_parameters(self):
        """权重初始化"""
        stdv = 1.0 / math.sqrt(self.state_size)
        for weight in self.parameters():
            weight.data.uniform_(-stdv, +stdv)

    def forward(self, input, state):
        """前向传播"""
        old_h, old_cell = state					# 旧细胞状态
        X = torch.cat([old_h, input], dim=1)	# 拼接出信号

        # Compute the input, output and candidate cell gates with one MM.
        gate_weights = F.linear(X, self.weights, self.bias)
        # Split the combined gate weight matrix into its components.
        gates = gate_weights.chunk(3, dim=1)

        input_gate = torch.sigmoid(gates[0])
        output_gate = torch.sigmoid(gates[1])
        # Here we use an ELU instead of the usual tanh.
        candidate_cell = F.elu(gates[2])

        # Compute the new cell state.
        new_cell = old_cell + candidate_cell * input_gate
        # Compute the new hidden state and output.
        new_h = torch.tanh(new_cell) * output_gate

        return new_h, new_cell
```

