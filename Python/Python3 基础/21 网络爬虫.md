## 1 最简单的爬虫
爬取百度首页源代码：

```python
# -*- coding:utf-8 -*-

import urllib.request

url = "http://www.baidu.com"
page_info = urllib.request.urlopen(url).read()
page_info = page_info.decode('utf-8')
print(page_info)
```

* `urllib` - 这是爬虫程序的重要模块，可以非常方便地模拟浏览器访问互联网，处理 URL。

>`urllib` is a package that collects several modules for working with URLs:
>
>>`urllib.request` for opening and reading URLs
>
>>`urllib.error` containing the exceptions raised by urllib.request
>
>>`urllib.parse` for parsing URLs
>
>>`urllib.robotparser` for parsing robots.txt files

* `urllib.request` - 这是`urllib` 的一个子模块，可以打开和处理一些复杂的网址。

>The `urllib.request` module defines functions and classes which help in opening URLs (mostly HTTP) in a complex world — basic and digest authentication, redirections, cookies and more.

* `urllib.request.urlopen()` - 该方法能打开一个 URL，返回一个 `http.client.HTTPResponse` 对象，

>Open the URL url, which can be either a string or a Request object.

>通过该对象的 `read()` 方法可以获得 `response body`，`decode('utf-8')` 解码后可以通过 `print()` 输出

## 2 模拟浏览器爬取信息
有些网站会判断访问是否带有头文件来鉴别该访问是否为爬虫，故将模拟浏览器作为反爬取的策略。

添加浏览器头信息然后爬取网页：

```python
# -*- coding:utf-8 -*-

from urllib import request

url = "http://www.baidu.com"
# 模拟真实浏览器进行访问
headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.92 Safari/537.36'}
page = request.Request(url, headers=headers)
page_info = request.urlopen(url).read().decode('utf-8')
print(page_info)
```

首先通过浏览器查看网页，具体方法是`F12`打开浏览器开发者模式，在`Internet`选项卡下点击当前网页文件，之后显示的子窗口中点击`headers`项，找到`User-Agent`的数据。

有两种方法向爬虫添加头信息：一是构造 `urllib.request.Request` 对象，二是通过该对象的 `add_header()` 方法

* `urllib.request.Request(url, headers={})` - 这是一个类，可以接收一个 URL 和 headers 字典类型的头信息数据，构造出 `urllib.request.Request` 对象

> class urllib.request.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None)
>
>>This class is an abstraction of a URL request.
>
>>url should be a string containing a valid URL.

* `urllib.request.Request.add_header(key, val)` - `urllib.request.Request` 对象的一个实例方法，用途向该对象添加一个头信息

> Add another header to the request. Headers are currently ignored by all handlers except HTTP handlers, where they are added to the list of headers sent to the server. Note that there cannot be more than one header with the same name, and later calls will overwrite previous calls in case the key collides. Currently, this is no loss of HTTP functionality, since all headers which have meaning when used more than once have a (header-specific) way of gaining the same functionality using only one header.

## 3 使用 Beautiful Soup处理数据
Beautiful Soup 是一个可以从HTML或XML文件中提取数据的Python库。

下面是一个获取网页文章标题的例子，用到一个 `find_all()` 方法：

```python
# -*- coding:utf-8 -*-

from urllib import request
from bs4 import BeautifulSoup

url = "http://www.baidu.com"
# 模拟真实浏览器进行访问
headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.92 Safari/537.36'}
page = request.Request(url, headers=headers)
page_info = request.urlopen(url).read().decode('utf-8')
print(page_info)

# 将获取到的内容转换成 Beautiful Soup 格式，并将 html.parser 作为解析器
soup = BeautifulSoup(page_info, 'html.parser')

# 以格式化形式输出html
print(soup.prettify())
# 查找所有 a 标签的 class='title' 进行访问
titles = soup.find_all('a', 'title')
# 输出所有结果到 string
for title in titles:
    print(title.string)
```

Beautiful Soup 里面有大量的处理网页数据的方法，具体内容可以查看官方帮助文档。

关于 Beautiful Soup 的解析器：

<table>
<thead>
<tr>
<th>解析器</th>
<th>使用方法</th>
<th>优势</th>
<th>劣势</th>
</tr>
</thead>
<tbody>
<tr>
<td>Python标准库</td>
<td>BeautifulSoup(markup, "html.parser")</td>
<td>(1)Python的内置标准库 (2)执行速度适中 (3)文档容错能力强</td>
<td>Python 2.7.3 or 3.2.2 前的版本中文容错能力差</td>
</tr>
<tr>
<td>lxml HTML 解析器</td>
<td>BeautifulSoup(markup, "lxml")</td>
<td>(1)速度快 (2)文档容错能力强</td>
<td>需要安装C语言库</td>
</tr>
<tr>
<td>lxml XML 解析器</td>
<td>BeautifulSoup(markup, ["lxml", "xml"])  OR  BeautifulSoup(markup, "xml")</td>
<td>(1)速度快 (2)唯一支持XML的解析器</td>
<td>需要安装C语言库</td>
</tr>
<tr>
<td>html5lib</td>
<td>BeautifulSoup(markup, "html5lib")</td>
<td>(1)最好的容错性 (2)以浏览器的方式解析文档 (3)生成HTML5格式的文档</td>
<td>(1)速度慢 (2)不依赖外部扩展</td>
</tr>
</tbody>
</table>

## 4 将数据存储到本地
### 4.1 存储本地文件
Python3 内置了读写文件的函数：`open`，详细内容参考 “Python3基础 - 08 IO编程”。

```python
# -*- coding:utf-8 -*-

from urllib import request
from bs4 import BeautifulSoup

url = "https://www.baidu.com/"
# 模拟真实浏览器进行访问
headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.92 Safari/537.36'}
page = request.Request(url, headers=headers)
page_info = request.urlopen(url).read().decode('utf-8')
print(page_info)

# 将获取到的内容转换成 Beautiful Soup 格式，并将 html.parser 作为解析器
soup = BeautifulSoup(page_info, 'html.parser')

# 查找所有 a 标签的 class='title' 进行访问
titles = soup.find_all('a')
try:
    # 以只写的方式打开/创建一个名为 titles 的 txt 文件
    file = open(r'E:\titles.txt', 'w')
    for title in titles:
        # 见数据写入 txt 文件
        file.write(title.string + '\n')
finally:
    if file:
        file.close
```

注：也可以使用 `with ...` 语句进行文件读写。

## 4.2 图片的储存
首先观察图片目标网页图片格式，建立正确的提取方式，下面是一个例子：

```python
# -*- coding:utf-8 -*-

from urllib import request
from bs4 import BeautifulSoup
import re, time

url = "http://www.jianshu.com/"
# 模拟真实浏览器进行访问
headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.92 Safari/537.36'}
page = request.Request(url, headers=headers)
page_info = request.urlopen(url).read().decode('utf-8')
print(page_info)

# 将获取到的内容转换成 Beautiful Soup 格式，并将 html.parser 作为解析器
soup = BeautifulSoup(page_info, 'html.parser')

# 查找所有 img 标签
links = soup.find_all('img', "orign_image zh-lightbox-thumb", src=re.compile(r'.jpg$'))
# 设置当前保存路径
local_path = r'E"\Pic'
for link in links:
    print(link.attrs['src'])
    # 保存链接并命名，time 防止命名冲突
    request.urlretrieve(link.attrs['src'], local_path+r'\%s.jpg' % time.time)
```

原网页中图片在 `img` 标签中，`class` 值为 `origin_image zh-lightbox-thumb`， 再通过正则项过滤。

提取出链接后，通过 `request.urlretrieve` 来将所有链接保存到本地。

>urllib.request.urlretrieve(url, filename=None, reporthook=None, data=None)
>
>>Copy a network object denoted by a URL to a local file. If the URL points to a local file, the object will not be copied unless filename is supplied. Return a tuple (filename, headers) where filename is the local file name under which the object can be found, and headers is whatever the info() method of the object returned by urlopen() returned (for a remote object). Exceptions are the same as for urlopen().

## 5 数据库存储爬取信息
将爬取到的数据存储到数据库中，便于后期分析利用。

这里数据库选择 MySQL，采用 `pymysql` 这个第三方来处理 Python 和 MySQL 数据库的存取。

先配置 Python 连接 MySQL 数据库的信息：

```mysql
# 配置 MySQL 连接信息
da_config = {
    'host' : '127.0.0.1',
    'port' : 3306,
    'user' : 'Infuny',
    'password' : 'asdfgh',
    'db' : 'puresakura'
}
```

然后在 MySQL的建立名为 “puresakura” 的数据库，建立一张表，这里命名为 “titles”，字段分别为 `id(int自增)`，`title(varchar)`，`url(varchar)`

下面进行数据库的操作，思路为：

获取数据库连接（connection） -> 获得游标（cursor） -> 执行 sql 语句 -> 提交事务（commit） -> 关闭数据库连接（close）

```python
# -*- coding:utf-8 -*-

from urllib import request
from bs4 import BeautifulSoup
import pymysql

# 配置 MySQL 连接信息
db_config = {
    'host' : '127.0.0.1',
    'port' : 3306,
    'user' : 'Infuny',
    'password' : 'asdfgh',
    'db' : 'puresakura',
    'charset' : 'utf-8'
}

# 获得数据库连接
connection = pymysql.connect(**db_config)

url = "http://www.jianshu.com/"
# 模拟浏览器头
headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.92 Safari/537.36'}
page = request.Request(url, headers=headers)
page_info = request.urlopen(url).read().decode('utf-8')
# 将获取到的内容转换成 Beautiful Soup 格式，并将 html.parser 作为解析器
soup = BeautifulSoup(page_info, 'html.parser')
# 查找所有 img 标签
links = soup.find_all('a', 'title')

# 存储到数据库
try:
    with connection.cursor() as cursor:
        sql = 'insert into titles(title, url) values(%s, %s)'
        for link in links:
            # 执行 sql 语句
            cursor.execute(sql, (link.string, r'http://www.jianshu.com'+link.attrs['href']))
    # 事务提交
    connection.commit()
finally:
    # 关闭数据库
    connection.close()
```
