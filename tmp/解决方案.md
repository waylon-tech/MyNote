### 1 使用 BDSP

#### 1 使用 hdfs url 格式

<img src="img/image-20220908232108776.png" alt="image-20220908232108776" style="zoom:50%;" />

```scala
"hdfs://<ip:port>/user/Cloudera/Test"
```

来源：https://sparkbyexamples.com/spark/spark-read-write-files-from-hdfs-txt-csv-avro-parquet-json/

#### 2 去掉 hdfs 前缀

<img src="img/image-20220908232314288.png" alt="image-20220908232314288" style="zoom:50%;" />

<img src="img/image-20220908232344025.png" alt="image-20220908232344025" style="zoom:50%;" />

来源：https://stackoverflow.com/questions/41706343/url-for-hdfs-file-system

#### 3 查找 hdfs url 的方法

<img src="img/image-20220908232559362.png" alt="image-20220908232559362" style="zoom:50%;" />

<img src="img/image-20220908232938355.png" alt="image-20220908232938355" style="zoom:50%;" />



---

#### hadoop 配置文件详解

![img](img/v2-dff2be66297ddbf26c60cab2b3e1d3be_1440w.jpg)

1. `core-site.xml`

![img](img/v2-05696246e7edb924addab785311809ec_1440w.jpg)

2. `hdfs-site.xml`

![img](img/v2-a8eeb87a945f90c128824f776e4b8845_1440w.jpg)

3. `mapred-site.xml`

 ![img](img/v2-5410e59ef4ae921c5ed56cc3be3e5ecf_1440w.jpg)

4. `yarn-site.xml`

![img](img/v2-789f67bb082eea51849ed8a339eeb8e0_1440w.jpg)

### 2 尝试更小的数据

### 3 直接用 Spark 分布式存储

<img src="img/image-20220909082434861.png" alt="image-20220909082434861" style="zoom:50%;" />

来源：https://stackoverflow.com/questions/33174443/how-to-save-a-spark-dataframe-as-csv-on-disk

**使用 parquet**

https://stackoverflow.com/questions/57155855/how-we-save-a-huge-pyspark-dataframe

<img src="img/image-20220909081739925.png" alt="image-20220909081739925" style="zoom:50%;" />

来源：https://stackoverflow.com/questions/50754517/df-topandas-failed-to-locate-the-winutils-binary-in-the-hadoop-binary-path

**曲线救国**

<img src="img/image-20220909081922474.png" alt="image-20220909081922474" style="zoom:50%;" />

来源：https://stackoverflow.com/questions/69900672/conversion-issue-for-spark-dataframe-to-pandas

### 4 在 spark 上使用 pyspark api

**Pandas API on Spark**

https://spark.apache.org/docs/latest//api/python/user_guide/pandas_on_spark/index.html
